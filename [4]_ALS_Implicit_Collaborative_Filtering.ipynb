{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[4] ALS Implicit Collaborative Filtering.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlaricobar/Machine-Learning-Course/blob/master/%5B4%5D_ALS_Implicit_Collaborative_Filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ArrnmdH9Ep3V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Filtro Colaborativo Implícito con ALS**\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/2400/1*6y8nVYtQef_jW-ZeKY2zMw.png\" />"
      ]
    },
    {
      "metadata": {
        "id": "4YNh6qQaFY0w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Papers de Referencia:\n",
        "\n",
        "1.   Collaborative Filtering for Implicit Feedback Datasets: http://yifanhu.net/PUB/cf.pdf\n",
        "2.   \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WDKQicjiGVP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Contenido:\n",
        "\n",
        "\n",
        "1.   Introducción\n",
        "2.   Implícito vs Explícito\n",
        "3.   El dataset\n",
        "4.   Alternating least squares\n",
        "5.   Ítems similares\n",
        "6.   Hacer recomendaciones"
      ]
    },
    {
      "metadata": {
        "id": "mI4H2v1pHM-O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Introducción\n",
        "\n",
        "\n",
        "En este notebook voy a implementar un algoritmo de recomendación implícito. Buscaré ser capaz de encontrar ítems similares y dar recomendaciones a los usuarios. En este notebook cubriremos la parte teórica, matemática así como algunas implementaciones en python.\n",
        "\n",
        "Debido a que nos enfocaremos en la técnica del filtro colaborativo, solo nos enfocaremos en los ítems, usuarios y en los ítems con los que los usuarios han interactuado."
      ]
    },
    {
      "metadata": {
        "id": "fkkGft5_ITXG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Datos Explícitos vs Implícitos\n",
        "\n",
        "### 2.1. Datos Explícitos\n",
        "\n",
        "Son aquellos conjuntos de datos en los que se cuenta con un concepto de rating que hace referencia a la valoración que le da un usuario a un producto en específico, por ejemplo en los datasets de Netflix o MovieLens encontraremos ratings que se encuentran entre 1 a 5. Con este dato podemos saber que tanto a un usuario le gusta o no un ítem lo que es genial, sin embargo es difícil obtener estos datos. Esto es debido a que los usuarios por lo general no utilizan estas opciones de valoración de productos.\n",
        "\n",
        "### 2.2. Datos Implícitos\n",
        "\n",
        "Son aquellos datos que extraemos del comportamiento de los usuarios, sin ratings ni acciones específicas necesarias. Por ejemplo podrían ser los ítems que un usuario ha comprado, cuántas veces los usuarios han reproducido una canción o visto una película, cuánto tiempo ha pasado un usuario leyendo un artñiculo, etc. La ventaja es que tenemos muchos más datos, sin embargo la desventaje es que son más ruidosos y no siempre es evidente lo que significan.\n",
        "\n",
        "Por ejemplo, con los ratings de estrellas sabemos que 1 significa que a un usuario no le gusta un ítem y 5 que realmente le encanta. Con la reproducción de canciones puede ser que un usuario haya reproducido una canción y la haya odiado, o le haya encantado, o en algún lugar intermedio, y si no reproduciera una canción, es posible que no les guste o que le encantaría conocer para reproducirla.\n",
        "\n",
        "En esta oportunidad nos centraremos en lo que sabemos que el usuario ha consumido y en la confianza que tenemos en si les gusta o no algún ítem en particular. **Por ejemplo, podemos medir la frecuencia con la que reproducen una canción y asumir una mayor confianza si la han escuchado 500 veces en comparación con una vez.**\n",
        "\n",
        "Las recomendaciones implícitas se están convirtiendo en un aspecto cada vez más importante de muchos sistemas de recomendación a medida que crece la cantidad de datos implícitos. Por ejemplo, el desafío original de Netflix se centró solo en datos explícitos, pero ahora están confiando cada vez más en señales implícitas. Lo mismo ocurre con Hulu, Spotify, Etsy y muchos otros."
      ]
    },
    {
      "metadata": {
        "id": "wNDIPGEJPTGR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. El dataset\n",
        "\n",
        "En esta oportunidad se utilizará el dataset de **Lastfm** que contiene el comportamiento de escucha de 360,000 usuarios.\n",
        "\n",
        "Además, contiene la identificación del usuario, la identificación del artista, el nombre de los artistas y el número de veces que un usuario escuchó a un artista determinado. La descarga también contiene un archivo con edades de usuario, el género y su nacionalidad, etc., pero no lo usaremos por el momento."
      ]
    },
    {
      "metadata": {
        "id": "_HaSu68faocP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Alternating Least Squares\n",
        "\n",
        "Alternating Least Squares (ALS) es el modelo que usaremos para ajustar nuestros datos y encontrar similitudes. Pero antes de enfocarnos en cómo funciona, deberíamos analizar algunos de los conceptos básicos de la factorización matricial, que es lo que pretendemos utilizar en el ALS.\n",
        "\n",
        "### 4.1. Factorización de Matrices\n",
        "\n",
        "La idea es básicamente tomar una matriz grande (o potencialmente enorme) y factorizarla en una representación más pequeña de la matriz original. Puede pensarlo de la misma manera que tomaríamos un gran número y lo dividiríamos en dos números primos mucho más pequeños. Terminamos con dos o más matrices de dimensiones inferiores cuyo producto es igual al original.\n",
        "\n",
        "Cuando hablamos de filtrado colaborativo para sistemas de recomendación, queremos resolver el problema de nuestra matriz original con millones de dimensiones diferentes, pero nuestros \"gustos\" no son tan complejos. Incluso si he visto cientos de artículos, podrían expresar un par de gustos diferentes. Aquí podemos utilizar realmente la factorización de matriz para reducir matemáticamente la dimensionalidad de nuestra matriz original de \"todos los usuarios por todos los elementos\" en algo mucho más pequeño que represente \"todos los elementos por algunas dimensiones de gusto\" y \"todos los usuarios por algunas dimensiones de gusto\". Estas dimensiones se denominan características latentes u ocultas y las aprendemos de nuestros datos.\n",
        "\n",
        "Hacer esta reducción y trabajar con menos dimensiones hace que sea mucho más eficiente desde el punto de vista informático y, a la vez, nos proporciona mejores resultados, ya que podemos razonar acerca de los elementos en este \"espacio del gusto\" más compacto.\n",
        "\n",
        "**Si podemos expresar a cada usuario como un vector de sus valores de gusto, y al mismo tiempo expresar cada elemento como un vector de los gustos que representan. Puedes ver que podemos hacer una recomendación muy fácilmente**. Esto también nos da la posibilidad de encontrar conexiones entre usuarios que no tienen elementos específicos en común pero que comparten gustos comunes.\n",
        "\n",
        "Ahora debe notarse que no tenemos idea de cuáles son realmente estas características o gustos. No podremos etiquetarlos como \"rock\" o \"de ritmo rápido\" o \"con Jay-Z\". No reflejan necesariamente metadatos reales.\n",
        "\n",
        "\n",
        "### 4.1. Factorización de Matrices en Datos Implícitos\n",
        "\n",
        "Hay diferentes formas de factorizar una matriz, como la descomposición del valor singular (SVD) o el análisis semántico latente probabilístico (PLSA) si estamos tratando con datos explícitos.\n",
        "\n",
        "Con los datos implícitos, la diferencia radica en cómo manejamos todos los datos faltantes en nuestra muy escasa matriz. Para datos explícitos, los tratamos como campos desconocidos a los que debemos asignarles una calificación pronosticada. Pero implícitamente no podemos simplemente asumir lo mismo, ya que también hay información en estos valores desconocidos. **Como se indicó anteriormente, no sabemos si un valor faltante significa que al usuario no le gustó algo, o si significa que lo ama pero simplemente no lo sabe. Básicamente necesitamos alguna forma de aprender de los datos faltantes. Así que necesitaremos un enfoque diferente para llevarnos allí.**\n",
        "\n",
        "#### Volver a ALS\n",
        "\n",
        "ALS es un proceso de optimización iterativo en el que, en cada iteración, intentamos acercarnos cada vez más a una representación factorizada de nuestros datos originales.\n",
        "\n",
        "Tenemos nuestra matriz original R de tamaño u x i con nuestros usuarios, artículos y algún tipo de información de retroalimentación (o feedback). Luego queremos encontrar una manera de convertir eso en una matriz con usuarios y características ocultas de tamaño u x f y una con elementos y características ocultas de tamaño f x i. En U y V tenemos pesos sobre cómo se relaciona cada usuario / elemento con cada característica. Lo que hacemos es calcular U y V para que su producto se aproxime a R lo más cerca posible: R ≈ U x V.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/2400/1*ygHEXIhg5FtkSD3UQaldgw.png\" />\n",
        "\n",
        "Asignando aleatoriamente los valores en U y V y utilizando mínimos cuadrados de forma iterativa, podemos llegar a qué pesos producen la mejor aproximación de R. El enfoque de mínimos cuadrados en sus formas básicas significa ajustar una línea a los datos, midiendo la suma de las distancias al cuadrado. Todos los puntos apuntan a la línea e intentan obtener un ajuste óptimo minimizando este valor.\n",
        "\n",
        "Con el enfoque de mínimos cuadrados alternos usamos la misma idea, pero alternamos iterativamente entre la optimización de U y la fijación de V y viceversa. Hacemos esto para que cada iteración se acerque a R = U x V.\n",
        "\n",
        "El enfoque que vamos a utilizar con nuestro conjunto de datos implícitos es el que se describe en el filtrado colaborativo para conjuntos de datos de retroalimentación implícita de Hu, Korenand y Volinsky (y utilizado por Facebook y Spotify). Su solución es muy sencilla, así que solo voy a explicar la idea general y la implementación, pero definitivamente debería leerla.\n",
        "\n",
        "Su solución es fusionar la preferencia (p) para un ítem con la confianza (c) que tenemos para esa preferencia. **Comenzamos con los valores faltantes como una preferencia negativa con un valor de confianza bajo y los valores existentes una preferencia positiva pero con un valor de confianza alto.** Podemos usar algo como el conteo de reproducciones, el tiempo dedicado a una página o alguna otra forma de interacción como base para calcular nuestra confianza."
      ]
    },
    {
      "metadata": {
        "id": "xD5fCQ8imk0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Establecemos la preferencia (p):\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*fzBWtw4JtADMA_PjI8fUjA.png\" />\n",
        "\n",
        "Básicamente nuestra preferencia es una representación binaria de nuestros datos de retroalimentación r. Si la respuesta es mayor que cero, la configuramos en 1. Tiene sentido.\n",
        "\n",
        "*   La confianza (c) se calcula de la siguiente manera:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*1rnDwWv6upHMQAeoFI_izQ.png\" />\n",
        "\n",
        "Aquí, la confianza se calcula utilizando la magnitud de r (los datos de retroalimentación o feedback), lo que nos da una confianza mayor cuanto más veces un usuario ha reproducido, visto o hecho clic en un elemento. La tasa a la que aumenta nuestra confianza se establece a través de un factor de escala lineal α. También agregamos 1, por lo que tenemos una confianza mínima incluso si α x r es igual a cero.\n",
        "\n",
        "Esto también significa que incluso si solo tenemos una interacción entre un usuario y un elemento, la confianza será mayor que la de los datos desconocidos dado el valor α. **En el documento encontraron que α = 40 funcionaba bien y en algún lugar entre 15 y 40 funcionaron para mí.**\n",
        "\n",
        "El objetivo ahora es encontrar el vector para cada usuario (xu) y el elemento (yi) en las dimensiones de la función, lo que significa que queremos minimizar la siguiente función de pérdida:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*i6-tSA97cOAGVldVUsiKHw.png\" />\n",
        "\n",
        "Como señala el paper, si solucionamos los factores de usuario o los factores de elemento, podemos calcular un mínimo global. La derivada de la ecuación anterior nos brinda la siguiente ecuación para minimizar la pérdida de nuestros usuarios:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*YKevKqOWHKGiKNGMDT8Rnw.png\">\n",
        "\n",
        "Y esto para minimizarlo para nuestros artículos:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*mbiRya9U7A91Q32jMWNQwA.png\">\n",
        "\n",
        "Un paso más es que al darse cuenta de que el producto de la transpuesta de Y, Cu e Y se pueden desglosar como se muestra a continuación:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*Spm4pEFTMisdAv5CNGOjfA.png\">\n",
        "\n",
        "Ahora tenemos Y-transpuesta.Y y X-transpuesta.X independientes de u e i, lo que significa que podemos calcularlo y hacer que el cálculo sea mucho menos intensivo. Así que con eso en mente, nuestras ecuacionaes finales para el usuario e ítem son:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*Ppl1LxSu3zJsLXtck8NMRQ.jpeg\">\n",
        "\n",
        "\n",
        "*   X e Y: Nuestras matrices de usuarios y elementos inicializados aleatoriamente. Estos se actualizarán alternativamente.\n",
        "*   Cu y Ci: Nuestros valores de confianza.\n",
        "*   λ: Regularizador para reducir el sobreajuste (estamos usando 0.1).\n",
        "*   p (u) y p (i): la preferencia binaria para un elemento. Una si conocemos la preferencia y cero si no la conocemos.\n",
        "*   I: la matriz de identidad. Matriz cuadrada con unos en diagonal y ceros en cualquier otra parte.\n",
        "\n",
        "Al iterar entre las dos ecuaciones anteriores, llegamos a una matriz con vectores de usuario y otra con vectores de elementos que luego podemos usar para generar recomendaciones o encontrar similitudes.\n"
      ]
    },
    {
      "metadata": {
        "id": "VaiyllQusT-S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Artículos similares\n",
        "\n",
        "\n",
        "Para calcular la similitud entre los elementos, calculamos el punto-producto entre nuestros vectores de elementos y su transposición. Entonces, si queremos que artistas similares digan Joy Division, tomamos el producto punto entre todos los vectores de elementos y la transposición del vector de elementos de Joy Division. Esto nos dará la puntuación de similitud:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*aeQO6uL29wzQy8C1pz6Wwg.png\">\n",
        "\n",
        "## Haciendo recomendaciones\n",
        "\n",
        "\n",
        "Para hacer recomendaciones para un usuario determinado, tomamos un enfoque similar. Aquí calculamos el producto de punto entre nuestro vector de usuario y la transposición de nuestros vectores de elementos. Esto nos da una puntuación de recomendación para nuestro usuario y cada elemento:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*UGxdsKnvkekwW0IQNrj1ig.png\">"
      ]
    },
    {
      "metadata": {
        "id": "9IeTVisGtRkh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Ok, vamos a escribirlo!\n",
        "\n",
        "\n",
        "Primero importaremos las librerías que necesitamos y cargaremos el conjunto de datos de lastfm. También tendremos que hacer algunos arreglos para que los datos tomen la forma que queremos."
      ]
    },
    {
      "metadata": {
        "id": "SpZL9DQ_vWHH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TEdw683sxEFB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_data.info(null_counts=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80zAiUg1EkWn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Luego creamos nuestra matriz dispersa R de tamaño usuarios x ítems.  El uso de una matriz dispersa nos permite almacenar solo los valores que están realmente allí y no todos los que faltan (que es aproximadamente el 99% del conjunto de datos).\n",
        "\n",
        "Ahora que tenemos nuestros datos preparados y listos para comenzar, podemos comenzar con una primera implementación de nuestra función ALS implícita.\n",
        "\n",
        "Comenzamos calculando la confianza para todos los usuarios y elementos, creamos nuestras matrices X e Y para mantener nuestros vectores de usuario y elemento y asignamos los valores al azar. También precomputamos nuestras diagonales."
      ]
    },
    {
      "metadata": {
        "id": "7cURCgYGz385",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def implicit_als(sparse_data, alpha_val=40, iterations=10, lambda_val=0.1, features=10):\n",
        "  \n",
        "  \"\"\" \n",
        "  Implementación del Alternating Least Squares con datos implícitos. De forma iterativa se\n",
        "  calcula los vectores de usuario (x_u) e ítems (y_i) usando las siguientes formulas:\n",
        " \n",
        "    x_u = ((Y.T*Y + Y.T*(Cu - I) * Y) + lambda*I)^-1 * (X.T * Cu * p(u))\n",
        "    y_i = ((X.T*X + X.T*(Ci - I) * X) + lambda*I)^-1 * (Y.T * Ci * p(i))\n",
        " \n",
        "  Argumentos:\n",
        "      sparse_data (csr_matrix): Nuestra matriz dispersa usuario-por-ítem\n",
        "\n",
        "      alpha_val (int): El ratio en el que incrementaremos nuestra confianza de preferencia\n",
        "      con más interacciones.\n",
        "\n",
        "      iterations (int): Cuántas veces alternaremos entre corregir y actualizar los vectores\n",
        "      de usuarios e ítems.\n",
        "\n",
        "      lambda_val (float): Valor de regularización.\n",
        "\n",
        "      features (int): Cuántas características latentes queremos calcular.\n",
        "  \n",
        "  Retorna:\n",
        "      X (csr_matrix): Vector de usuarios del tamaño usuarios-por-características\n",
        "\n",
        "      Y (csr_matrix): Vector de ítems del tamaño ítems-por-características\n",
        "  \"\"\"\n",
        "  \n",
        "  # Calcular la confianza por cada valor en nuestro dataset\n",
        "  confidence = sparse_data * alpha_val\n",
        "\n",
        "  # Obtener el tamaño de los vectores de usuarios e ítems\n",
        "  user_size, item_size = sparse_data.shape\n",
        "\n",
        "  # Creamos nuestro vector de usuarios X del tamaño usuarios-por-características,\n",
        "  # el vector de ítmes Y del tamaño ítems-por-características y les asignamos valores aleatorios\n",
        "  X = sparse.csr_matrix(np.random.normal(size = (user_size, features)))\n",
        "  Y = sparse.csr_matrix(np.random.normal(size = (item_size, features)))\n",
        "\n",
        "  # Precalculamos I y lambda * I\n",
        "  X_I = sparse.eye(user_size)\n",
        "  Y_I = sparse.eye(item_size)\n",
        "\n",
        "  I = sparse.eye(features)\n",
        "  lI = lambda_val * I\n",
        "\n",
        "  \"\"\" Continuación de la función implicit_als\"\"\"\n",
        "\n",
        "  # Iniciamos el bucle principal. Por cada iteración calculamos X y luego Y\n",
        "  for i in range(iterations):\n",
        "    print('Iteración %d de %d' % (i+1, iterations))\n",
        "\n",
        "    # Precalculamos Y-transpuesta-Y e X-transpuesta-X\n",
        "    yTy = Y.T.dot(Y)\n",
        "    xTx = X.T.dot(X) \n",
        "\n",
        "    # Iterar por todos los usuarios\n",
        "    for u in range(user_size):\n",
        "      # Obtener el registro de Usuario\n",
        "      u_row = confidence[u,:].toarray()\n",
        "\n",
        "      # Calculamos la preferencia binaria p(u)\n",
        "      p_u = u_row.copy()\n",
        "      p_u[p_u != 0] = 1.0\n",
        "\n",
        "      # Calculamos Cu y Cu + I\n",
        "      CuI = sparse.diags(u_row, [0])\n",
        "      Cu = CuI + Y_I\n",
        "\n",
        "      # Put it all together and compute the final formula\n",
        "      yT_CuI_y = Y.T.dot(CuI).dot(Y)\n",
        "      yT_Cu_pu = Y.T.dot(Cu).dot(p_u.T)\n",
        "      X[u] = spsolve(yTy + yT_CuI_y + lI, yT_Cu_pu)\n",
        "\n",
        "    for i in xrange(item_size):\n",
        "\n",
        "      # Obtener la columna ítem y la transpuesta.\n",
        "      i_row = confidence[:,i].T.toarray()\n",
        "\n",
        "      # Calcular la preferencia binaria p(i)\n",
        "      p_i = i_row.copy()\n",
        "      p_i[p_i != 0] = 1.0\n",
        "\n",
        "      # Calcular Ci y Ci - I\n",
        "      CiI = sparse.diags(i_row, [0])\n",
        "      Ci = CiI + X_I\n",
        "\n",
        "      # Ponlo todo junto y calcula la fórmula final.\n",
        "      xT_CiI_x = X.T.dot(CiI).dot(X)\n",
        "      xT_Ci_pi = X.T.dot(Ci).dot(p_i.T)\n",
        "      Y[i] = spsolve(xTx + xT_CiI_x + lI, xT_Ci_pi)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I6XSHENhbwtp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Luego llamamos a nuestra función para obtener nuestros vectores de usuario y vectores de ítems. Como verás, si intentas ejecutar esto con el conjunto de datos que estamos usando, te llevará mucho tiempo entrenar. Volveremos a cómo podemos acelerarlo más tarde, pero por ahora podemos intentarlo con una sola iteración en lugar de 20. O podemos dividir los datos sin procesar en algo más manejable, por ejemplo, solo las primeras 100.000 filas."
      ]
    },
    {
      "metadata": {
        "id": "wHcar7tez3_k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#user_vecs, item_vecs = implicit_als(data_sparse, iterations=2, features=20, alpha_val=40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d0SN2HT6cEX3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Entonces, cuando tengamos nuestro modelo entrenado, podemos comenzar a hacer algunas recomendaciones. Primero vamos a comenzar por encontrar algunos artistas similares a Jay-Z. Obtenemos la similitud al hablar del producto punto de nuestros vectores de elementos con el vector de elementos del artista."
      ]
    },
    {
      "metadata": {
        "id": "hMJaJmnmz4CO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#------------------------------\n",
        "# FIND SIMILAR ITEMS\n",
        "#------------------------------\n",
        "\n",
        "# Let's find similar artists to Jay-Z. \n",
        "# Note that this ID might be different for you if you're using\n",
        "# the full dataset or if you've sliced it somehow. \n",
        "item_id = 10277\n",
        "\n",
        "# Get the item row for Jay-Z\n",
        "item_vec = item_vecs[item_id].T\n",
        "\n",
        "# Calculate the similarity score between Mr Carter and other artists\n",
        "# and select the top 10 most similar.\n",
        "scores = item_vecs.dot(item_vec).toarray().reshape(1,-1)[0]\n",
        "top_10 = np.argsort(scores)[::-1][:10]\n",
        "\n",
        "artists = []\n",
        "artist_scores = []\n",
        "\n",
        "# Get and print the actual artists names and scores\n",
        "for idx in top_10:\n",
        "    artists.append(item_lookup.artist.loc[item_lookup.artist_id == str(idx)].iloc[0])\n",
        "    artist_scores.append(scores[idx])\n",
        "\n",
        "similar = pd.DataFrame({'artist': artists, 'score': artist_scores})\n",
        "\n",
        "print(similar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9uv88VSdz4E1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xQUZlgm7z4H6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's say we want to recommend artists for user with ID 2023\n",
        "user_id = 2023\n",
        "\n",
        "#------------------------------\n",
        "# GET ITEMS CONSUMED BY USER\n",
        "#------------------------------\n",
        "\n",
        "# Let's print out what the user has listened to\n",
        "consumed_idx = data_sparse[user_id,:].nonzero()[1].astype(str)\n",
        "consumed_items = item_lookup.loc[item_lookup.artist_id.isin(consumed_idx)]\n",
        "print consumed_items\n",
        "\n",
        "\n",
        "#------------------------------\n",
        "# CREATE USER RECOMMENDATIONS\n",
        "#------------------------------\n",
        "\n",
        "def recommend(user_id, data_sparse, user_vecs, item_vecs, item_lookup, num_items=10):\n",
        "    \"\"\"Recommend items for a given user given a trained model\n",
        "    \n",
        "    Args:\n",
        "        user_id (int): The id of the user we want to create recommendations for.\n",
        "        \n",
        "        data_sparse (csr_matrix): Our original training data.\n",
        "        \n",
        "        user_vecs (csr_matrix): The trained user x features vectors\n",
        "        \n",
        "        item_vecs (csr_matrix): The trained item x features vectors\n",
        "        \n",
        "        item_lookup (pandas.DataFrame): Used to map artist ids to artist names\n",
        "        \n",
        "        num_items (int): How many recommendations we want to return:\n",
        "        \n",
        "    Returns:\n",
        "        recommendations (pandas.DataFrame): DataFrame with num_items artist names and scores\n",
        "    \n",
        "    \"\"\"\n",
        "  \n",
        "    # Get all interactions by the user\n",
        "    user_interactions = data_sparse[user_id,:].toarray()\n",
        "\n",
        "    # We don't want to recommend items the user has consumed. So let's\n",
        "    # set them all to 0 and the unknowns to 1.\n",
        "    user_interactions = user_interactions.reshape(-1) + 1 #Reshape to turn into 1D array\n",
        "    user_interactions[user_interactions > 1] = 0\n",
        "\n",
        "    # This is where we calculate the recommendation by taking the \n",
        "    # dot-product of the user vectors with the item vectors.\n",
        "    rec_vector = user_vecs[user_id,:].dot(item_vecs.T).toarray()\n",
        "\n",
        "    # Let's scale our scores between 0 and 1 to make it all easier to interpret.\n",
        "    min_max = MinMaxScaler()\n",
        "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
        "    recommend_vector = user_interactions*rec_vector_scaled\n",
        "   \n",
        "    # Get all the artist indices in order of recommendations (descending) and\n",
        "    # select only the top \"num_items\" items. \n",
        "    item_idx = np.argsort(recommend_vector)[::-1][:num_items]\n",
        "\n",
        "    artists = []\n",
        "    scores = []\n",
        "\n",
        "    # Loop through our recommended artist indicies and look up the actial artist name\n",
        "    for idx in item_idx:\n",
        "        artists.append(item_lookup.artist.loc[item_lookup.artist_id == str(idx)].iloc[0])\n",
        "        scores.append(recommend_vector[idx])\n",
        "\n",
        "    # Create a new dataframe with recommended artist names and scores\n",
        "    recommendations = pd.DataFrame({'artist': artists, 'score': scores})\n",
        "    \n",
        "    return recommendations\n",
        "\n",
        "# Let's generate and print our recommendations\n",
        "recommendations = recommend(user_id, data_sparse, user_vecs, item_vecs, item_lookup)\n",
        "print(recommendations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-L8OvY6mz4KM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yhiKyee2z4Mp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nonzeros(m, row):\n",
        "  for index in range(m.indptr[row], m.indptr[row+1]):\n",
        "    yield m.indices[index], m.data[index]\n",
        "      \n",
        "      \n",
        "def implicit_als_cg(Cui, features=20, iterations=20, lambda_val=0.1):\n",
        "  user_size, item_size = Cui.shape\n",
        "  \n",
        "  X = np.random.rand(user_size, features) * 0.01\n",
        "  Y = np.random.rand(item_size, features) * 0.01\n",
        "  \n",
        "  Cui, Ciu = Cui.tocsr(), Cui.T.tocsr()\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "      print ('iteration %d of %d' % (iteration+1, iterations))\n",
        "      least_squares_cg(Cui, X, Y, lambda_val)\n",
        "      least_squares_cg(Ciu, Y, X, lambda_val)\n",
        "    \n",
        "  return sparse.csr_matrix(X), sparse.csr_matrix(Y)\n",
        "  \n",
        "  \n",
        "def least_squares_cg(Cui, X, Y, lambda_val, cg_steps=3):\n",
        "  users, features = X.shape\n",
        "\n",
        "  YtY = Y.T.dot(Y) + lambda_val * np.eye(features)\n",
        "\n",
        "  for u in range(users):\n",
        "\n",
        "      x = X[u]\n",
        "      r = -YtY.dot(x)\n",
        "\n",
        "      for i, confidence in nonzeros(Cui, u):\n",
        "          r += (confidence - (confidence - 1) * Y[i].dot(x)) * Y[i]\n",
        "\n",
        "      p = r.copy()\n",
        "      rsold = r.dot(r)\n",
        "\n",
        "      for it in range(cg_steps):\n",
        "          Ap = YtY.dot(p)\n",
        "          for i, confidence in nonzeros(Cui, u):\n",
        "              Ap += (confidence - 1) * Y[i].dot(p) * Y[i]\n",
        "\n",
        "          alpha = rsold / p.dot(Ap)\n",
        "          x += alpha * p\n",
        "          r -= alpha * Ap\n",
        "\n",
        "          rsnew = r.dot(r)\n",
        "          p = r + (rsnew / rsold) * p\n",
        "          rsold = rsnew\n",
        "\n",
        "      X[u] = x\n",
        "\n",
        "alpha_val = 15\n",
        "conf_data = (data_sparse * alpha_val).astype('double')\n",
        "user_vecs, item_vecs = implicit_als_cg(conf_data, iterations=20, features=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D_YdPzjaiaS6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Código de Filtrado Colaborativo con Tipo de Entrada Implícita"
      ]
    },
    {
      "metadata": {
        "id": "24AZoJ6WiovC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install implicit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ghOEkaHik78a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import implicit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sparse\n",
        "\n",
        "from google.colab import drive\n",
        "from scipy.sparse.linalg import spsolve\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qt2n7ElItzed",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TNqC_3vdt_YV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l /gdrive/'My Drive'/Tesis/Blogs/Data/Last.fm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u1SxtwqLMBZd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creación del Sistema de Recomendación con datos Implícitos"
      ]
    },
    {
      "metadata": {
        "id": "ZOPsOCTDk_KP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the data like we did before\n",
        "raw_data = pd.read_table('/gdrive/My Drive/Tesis/Blogs/Data/Last.fm/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv', usecols = [0, 2, 3], header=None,  names=['user', 'artist', 'plays'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMozGp1Utnss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drop NaN columns\n",
        "data = raw_data.dropna()\n",
        "data = data.copy()\n",
        "\n",
        "# Create a numeric user_id and artist_id column\n",
        "data['user'] = data['user'].astype(\"category\")\n",
        "data['artist'] = data['artist'].astype(\"category\")\n",
        "data['user_id'] = data['user'].cat.codes\n",
        "data['artist_id'] = data['artist'].cat.codes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aPzigcA0z4O8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The implicit library expects data as a item-user matrix so we\n",
        "# create two matricies, one for fitting the model (item-user) \n",
        "# and one for recommendations (user-item)\n",
        "sparse_item_user = sparse.csr_matrix((data['plays'].astype(float), (data['artist_id'], data['user_id'])))\n",
        "sparse_user_item = sparse.csr_matrix((data['plays'].astype(float), (data['user_id'], data['artist_id'])))\n",
        "\n",
        "# Initialize the als model and fit it using the sparse item-user matrix\n",
        "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
        "\n",
        "# Calculate the confidence by multiplying it by our alpha value.\n",
        "alpha_val = 15\n",
        "data_conf = (sparse_item_user * alpha_val).astype('double')\n",
        "\n",
        "#Fit the model\n",
        "model.fit(data_conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mzbmSMg3L0Uy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Identifica a tu Artista"
      ]
    },
    {
      "metadata": {
        "id": "vFGjfMQHLy-Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.loc[data[\"artist\"].apply(lambda a: \"nirvana\" in a.lower()), :].groupby([\"artist\", \"artist_id\"], as_index=False).agg({\"user\": \"count\"}).sort_values(by=\"user\", ascending=False).rename(columns={\"user\": \"q_users\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Iujq9X4L6iy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encuentra a los Similares"
      ]
    },
    {
      "metadata": {
        "id": "2kgfv7JElHgz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#---------------------\n",
        "# FIND SIMILAR ITEMS\n",
        "#---------------------\n",
        "\n",
        "# Find the 10 most similar to Jay-Z\n",
        "item_id = 199842 #Nirvana\n",
        "n_similar = 30\n",
        "\n",
        "# Use implicit to get similar items.\n",
        "similar = model.similar_items(item_id, n_similar)\n",
        "\n",
        "# Print the names of our most similar artists\n",
        "for item in similar:\n",
        "    idx, score = item\n",
        "    print (data.artist.loc[data.artist_id == idx].iloc[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Ogh6UaZL97c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recomendaciones de Usuarios"
      ]
    },
    {
      "metadata": {
        "id": "zCa37npfibfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#------------------------------\n",
        "# CREATE USER RECOMMENDATIONS\n",
        "#------------------------------\n",
        "\n",
        "# Create recommendations for user with id 2025\n",
        "user_id = 198823\n",
        "\n",
        "# Use the implicit recommender.\n",
        "recommended = model.recommend(user_id, sparse_user_item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PKyA-v3KNyjj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### ¿Que vió anteriormente?"
      ]
    },
    {
      "metadata": {
        "id": "NL17MT9FlMfY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.loc[data[\"user_id\"] == 198823]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jvy9sUYnOIHG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Sus recomendaciones"
      ]
    },
    {
      "metadata": {
        "id": "_z_63IG7Nwbw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "artists = []\n",
        "scores = []\n",
        "\n",
        "# Get artist names from ids\n",
        "for item in recommended:\n",
        "    idx, score = item\n",
        "    artists.append(data.artist.loc[data.artist_id == idx].iloc[0])\n",
        "    scores.append(score)\n",
        "\n",
        "# Create a dataframe of artist names and scores\n",
        "recommendations = pd.DataFrame({'artist': artists, 'score': scores})\n",
        "\n",
        "print (recommendations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kl8sqCpRODNF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}